{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "The below gets the data from the latest week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables on site:  2\n",
      "['team', 'spi', 'off.', 'def.', 'goal diff.', 'proj. pts.pts.', 'Every position', 'relegatedrel.', 'qualify for UCLmake UCL', 'win Premier Leaguewin league']\n"
     ]
    }
   ],
   "source": [
    "# This is a method of scraping table data from online (generally tagged in html as <table>)\n",
    "# Install packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Site URL ### CHANGE THIS TO GET DIFFERENT DATA\n",
    "url=\"https://web.archive.org/web/20220720134228/https://projects.fivethirtyeight.com/soccer-predictions/premier-league/\"\n",
    "\n",
    "# GET request to fetch the raw html content\n",
    "html_content = requests.get(url).text\n",
    "\n",
    "# Parse HTML code for the entire site\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "#print(soup.prettify())\n",
    "\n",
    "# Just checking how many tables there are on the page so we only get the one\n",
    "premleague = soup.find_all(\"table\")\n",
    "print(\"Number of tables on site: \", len(premleague))\n",
    "\n",
    "# For reproducibility, lets pretend there are more than one tables and we want just the first (the 0th index in Python speech)\n",
    "# Scrape the first table\n",
    "table1 = premleague[0]   # Change this to 1 if it recognises more than one table on the page\n",
    "# Head will form our column names \n",
    "body = table1.find_all(\"tr\")\n",
    "# Head values are teh first itesm of the body list\n",
    "head = body[2] # ie the 0th item is the header row #note we can change this to 2 to get the right headings\n",
    "bodyrows = body[3:] # every other item apart from the 0th make up the rest of the rows in the table # note we've changed this to 3rd item and beyond because 2nd item is the column\n",
    "\n",
    "# Unsure if this will work but lets see\n",
    "# Iterate through the head html code and make list of clean headings\n",
    "headings = []\n",
    "for item in head.find_all(\"th\"): # loop through all the th elements\n",
    "    # conver the the elements to text and strip \\n\n",
    "    item = (item.text).rstrip(\"\\n\")\n",
    "    # append clean column name to headings\n",
    "    headings.append(item)\n",
    "print(headings)\n",
    "# Issue is the table has two sets of headers - we need the second row.\n",
    "# If we change head to body[2], we get the right values...\n",
    "\n",
    "# Now to loop through the rest of the rows\n",
    "all_rows = [] # this is going to be a list for list of all rows\n",
    "for row_num in range(len(bodyrows)): # A row at a time\n",
    "    row = [] # to hold old entires for one row\n",
    "    for row_item in bodyrows[row_num].find_all(\"td\"): # loop through all row entries\n",
    "        # row_item.text removes the tags from the entries\n",
    "        # the following regex is to remove \\xa0 and \\n and comma from row_item.text\n",
    "        # xa0 encodes the flag, \\n is the newline and comma seperates thousands in numbers\n",
    "        aa = re.sub(\"(\\xa0)|(\\n)|,\",\"\",row_item.text)\n",
    "        # append aa to row - note one row entry is being appended\n",
    "        row.append(aa)\n",
    "    # append one row to all_rows\n",
    "    all_rows.append(row)\n",
    "\n",
    "# So now we can use the data on all_rowsa and heading to make a table\n",
    "# all_rows becomes our data and headings the column names\n",
    "df = pd.DataFrame(data=all_rows,columns=headings)\n",
    "df.head()\n",
    "\n",
    "# Extract date of update\n",
    "last_updated = soup.find(\"h2\", text=re.compile(r\"England\")).find_next_sibling(\"h3\").get_text(strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "There are two main problems:\n",
    "    1. The team name and the points are in the same column. We want to seperate that (should be easy enough to seperate because the end of the team name is the first number of the points)\n",
    "    2. We need each week and to join the data together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to team name and points in same column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/7wx_w44j3mx9t6gygp9d79000000gn/T/ipykernel_58695/918905966.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['team'] = df['team'].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "## There is probably a more elegant way to do this but this worked\n",
    "# Add a space between the team name and number\n",
    "df = df.assign(\n",
    "    team=df[\"team\"].str.extract(r'(\\D+)') + \" \" + df[\"team\"].str.extract(r'(\\d+)')\n",
    ")\n",
    "# Extract the numbers out of the teams column and create a new column called points\n",
    "df['points'] = df['team'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
    "# Delete remaining numbers in the team column\n",
    "df['team'] = df['team'].str.replace('\\d+', '')\n",
    "# And get rid of the trailing white space at the end\n",
    "df['team'] = df['team'].str.strip( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the date\n",
    "You can easily add the date to a column if you can extract it (like with last_updated). The challenge is how can you use regex to change a clumsy string (\"Updated Sept. 18, 2022 at 11:10 a.m.\") into YYYYMMDD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a column called 'date' which just inserts the value of the 'last updated' which we extracted earlier.\n",
    "df['date'] = last_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated July 19, 2022, at 2:32 p.m.\n"
     ]
    }
   ],
   "source": [
    "print(last_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then save as a specific DF and bring together later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it all together\n",
    "What we now need to do is take the dataframes together and merge them together. This is a very imperfect method since we don't have every week and sometimes our data extracts are at times where not all teams have played. So some jumps will seem more dramatic. The data doesn't include number of games played either?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([df1, df2, df3, df4, df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser as dparser\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "July19 = \"Updated July 19, 2022, at 2:32 p.m.\"\n",
    "Sep3 = \"Updated Sept. 3, 2022, at 9:28 a.m.\"\n",
    "Sep18 = \"Updated Sept. 18, 2022, at 11:10 a.m.\"\n",
    "Oct2 = \"Updated Oct. 2, 2022, at 1:28 p.m.\"\n",
    "Oct8 = \"Updated Oct. 8, 2022, at 2:24 p.m.\"\n",
    "\n",
    "def categorise(row):\n",
    "    if July19 in row['date']:\n",
    "        return '2022-07-19'\n",
    "    elif Sep3 in row['date']:\n",
    "        return '2022-09-03'\n",
    "    elif Sep18 in row['date']:\n",
    "        return '2022-09-18'\n",
    "    elif Oct2 in row['date']:\n",
    "        return '2022-10-02'\n",
    "    elif Oct8 in row['date']:\n",
    "        return '2022-10-08'\n",
    "    \n",
    "merged['date_final'] = merged.apply(lambda row: categorise(row),\n",
    "                              axis=1)\n",
    "\n",
    "merged['date_final'] = pd.to_datetime(merged['date_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3c115c4647df14733300d4bfb6d9b3a20eb93dbb013a11b96eac1a2b1e91b0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
